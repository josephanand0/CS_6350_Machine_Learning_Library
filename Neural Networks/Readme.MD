This section is about implementations for Neural Networks



This consists of implementations for Back propagation algorithm, Stochastic Gradient Descent algorithm, Stochastic Gradient Descent algorithm by initializing weights with 0 and activation functions (tanh, RELU).



To run the back propagation use this command: python3 backpropagation.py


To run the Stochastic Gradient Descent algorithm use this command: python3 sgd.py


To run the Stochastic Gradient Descent algorithm by initializing weights with 0 use this command: python3 sgdzero.py


To run the activation functions use this command: python3 activation.py